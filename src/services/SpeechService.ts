/**
 * üîí PRIVACY NOTICE
 * All speech processing runs locally on device using RunAnywhere SDK.
 * Audio is processed in memory and transcribed locally.
 * No audio data is sent to external servers.
 */

import { NativeModules, Platform, PermissionsAndroid } from 'react-native';
import { RunAnywhere } from '@runanywhere/core';

const { NativeAudioModule } = NativeModules;

// ============================================================================
// STT MODEL STATUS HELPERS
// NOTE: Model loading is handled by ModelService.downloadAndLoadSTT() in App.tsx
// ============================================================================

/**
 * Check if STT model is ready (async check with RunAnywhere SDK)
 */
export const checkSTTModelReady = async (): Promise<boolean> => {
  try {
    const modelInfo = await RunAnywhere.getModelInfo('sherpa-onnx-whisper-tiny.en');
    return !!modelInfo?.localPath;
  } catch (error) {
    console.log('[SpeechService] STT model check failed:', error);
    return false;
  }
};

export interface TranscriptionCallback {
  (text: string, timestamp: number): void;
}

export interface AudioLevelCallback {
  (level: number): void;
}

/**
 * SpeechService - Handles audio recording and speech-to-text
 */
export class SpeechService {
  private isRecording: boolean = false;
  private recordingPath: string | null = null;
  private recordingStartTime: number = 0;
  private audioLevelInterval: NodeJS.Timeout | null = null;
  private transcriptionInterval: NodeJS.Timeout | null = null;
  private transcriptionCallback: TranscriptionCallback | null = null;
  private audioLevelCallback: AudioLevelCallback | null = null;
  private lastTranscriptionTime: number = 0;

  /**
   * Request microphone permission (Android only)
   */
  async requestPermission(): Promise<boolean> {
    if (Platform.OS !== 'android') {
      console.log('[SpeechService] ‚úÖ Platform is iOS, permission auto-granted');
      return true;
    }

    try {
      console.log('[SpeechService] üé§ Requesting RECORD_AUDIO permission...');

      const granted = await PermissionsAndroid.request(
        PermissionsAndroid.PERMISSIONS.RECORD_AUDIO,
        {
          title: 'Microphone Permission',
          message: 'Latent needs access to your microphone to transcribe speech.',
          buttonNeutral: 'Ask Me Later',
          buttonNegative: 'Cancel',
          buttonPositive: 'OK',
        }
      );

      const isGranted = granted === PermissionsAndroid.RESULTS.GRANTED;
      console.log(`[SpeechService] ${isGranted ? '‚úÖ' : '‚ùå'} Permission ${granted}`);

      return isGranted;
    } catch (error) {
      console.error('[SpeechService] ‚ùå Permission error:', error);
      return false;
    }
  }

  /**
   * Start recording audio
   */
  async startRecording(
    onTranscription: TranscriptionCallback,
    onAudioLevel?: AudioLevelCallback
  ): Promise<boolean> {
    try {
      console.log('[SpeechService] üéôÔ∏è startRecording() called');

      // Check native module
      if (!NativeAudioModule) {
        console.error('[SpeechService] ‚ùå NativeAudioModule not available');
        return false;
      }
      console.log('[SpeechService] ‚úÖ NativeAudioModule available');

      // Request permission
      const hasPermission = await this.requestPermission();
      if (!hasPermission) {
        console.error('[SpeechService] ‚ùå Microphone permission denied');
        return false;
      }
      console.log('[SpeechService] ‚úÖ Microphone permission granted');

      // Start native recording
      console.log('[SpeechService] üìº Starting native recording...');
      const result = await NativeAudioModule.startRecording();

      this.isRecording = true;
      this.recordingPath = result.path;
      this.recordingStartTime = Date.now();
      this.transcriptionCallback = onTranscription;
      this.audioLevelCallback = onAudioLevel || null;

      console.log('[SpeechService] ‚úÖ Recording started');
      console.log('[SpeechService] üìÅ Recording path:', result.path);
      console.log('[SpeechService] ‚è∞ Recording start time:', this.recordingStartTime);

      // Start audio level polling
      if (onAudioLevel) {
        this.startAudioLevelPolling();
        console.log('[SpeechService] üéöÔ∏è Audio level polling started');
      }

      // Start continuous transcription (every 5 seconds)
      this.startContinuousTranscription();
      console.log('[SpeechService] üîÑ Continuous transcription started');

      return true;
    } catch (error) {
      console.error('[SpeechService] ‚ùå Error starting recording:', error);
      this.isRecording = false;
      return false;
    }
  }

  /**
   * Stop recording and transcribe
   */
  async stopRecording(): Promise<string | null> {
    try {
      if (!this.isRecording || !NativeAudioModule) {
        return null;
      }

      // Stop audio level polling
      this.stopAudioLevelPolling();

      // Stop continuous transcription
      this.stopContinuousTranscription();

      // Stop native recording
      console.log('[SpeechService] Stopping recording...');
      const result = await NativeAudioModule.stopRecording();

      this.isRecording = false;
      const audioPath = result.path || this.recordingPath;

      if (!audioPath) {
        console.error('[SpeechService] No audio path available');
        return null;
      }

      // Transcribe the audio
      console.log('[SpeechService] Transcribing audio...');
      const transcription = await this.transcribeAudio(audioPath);

      // Clean up
      this.recordingPath = null;
      this.transcriptionCallback = null;
      this.audioLevelCallback = null;

      return transcription;
    } catch (error) {
      console.error('[SpeechService] Error stopping recording:', error);
      this.isRecording = false;
      return null;
    }
  }

  /**
   * Cancel recording without transcribing
   */
  async cancelRecording(): Promise<void> {
    try {
      if (!this.isRecording || !NativeAudioModule) {
        return;
      }

      // Stop audio level polling
      this.stopAudioLevelPolling();

      // Stop continuous transcription
      this.stopContinuousTranscription();

      // Cancel native recording
      await NativeAudioModule.cancelRecording();

      this.isRecording = false;
      this.recordingPath = null;
      this.transcriptionCallback = null;
      this.audioLevelCallback = null;

      console.log('[SpeechService] Recording cancelled');
    } catch (error) {
      console.error('[SpeechService] Error cancelling recording:', error);
      this.isRecording = false;
    }
  }

  /**
   * Transcribe audio file using RunAnywhere STT
   */
  private async transcribeAudio(audioPath: string): Promise<string | null> {
    try {
      console.log('[SpeechService] üéØ transcribeAudio() called');
      console.log('[SpeechService] üìÅ Audio path:', audioPath);

      // Check if STT model is ready
      const modelReady = await checkSTTModelReady();
      if (!modelReady) {
        console.error('[SpeechService] ‚ùå STT model not ready');
        throw new Error('STT model not loaded. Please ensure the model is downloaded.');
      }

      console.log('[SpeechService] ‚úÖ STT model ready');

      // Use RunAnywhere.transcribe() API
      console.log('[SpeechService] ü§ñ Running STT inference...');
      const result = await RunAnywhere.transcribe(audioPath);

      const transcription = result.text || '';
      console.log('[SpeechService] ‚úÖ Transcription complete');
      console.log('[SpeechService] üìù Text length:', transcription.length, 'chars');
      console.log('[SpeechService] üìù Text:', transcription);

      // Call callback with transcription
      if (this.transcriptionCallback) {
        console.log('[SpeechService] üì§ Calling transcription callback');
        this.transcriptionCallback(transcription, Date.now());
      } else {
        console.log('[SpeechService] ‚ö†Ô∏è No transcription callback registered');
      }

      return transcription;
    } catch (error) {
      console.error('[SpeechService] ‚ùå Transcription error:', error);
      return null;
    }
  }

  /**
   * Start continuous transcription (every 5 seconds)
   */
  private startContinuousTranscription(): void {
    this.lastTranscriptionTime = Date.now();

    this.transcriptionInterval = setInterval(async () => {
      try {
        if (!this.isRecording || !this.recordingPath) {
          return;
        }

        const currentTime = Date.now();

        // Only transcribe if 5 seconds have passed since last transcription
        if (currentTime - this.lastTranscriptionTime >= 5000) {
          console.log('[SpeechService] üîÑ Performing continuous transcription...');
          console.log('[SpeechService] üìÅ Transcribing file:', this.recordingPath);

          // Check if STT model is ready
          const modelReady = await checkSTTModelReady();
          if (!modelReady) {
            console.warn('[SpeechService] ‚ö†Ô∏è STT model not ready, skipping transcription');
            return;
          }

          // Try to transcribe the current recording
          try {
            const result = await RunAnywhere.transcribe(this.recordingPath);
            const text = result.text || '';

            console.log('[SpeechService] üìù Transcription result length:', text.length, 'chars');

            if (text && text.length > 0 && this.transcriptionCallback) {
              console.log('[SpeechService] ‚úÖ Continuous transcription result:', text);
              this.transcriptionCallback(text, currentTime);
              this.lastTranscriptionTime = currentTime;
            } else {
              console.log('[SpeechService] ‚ö†Ô∏è Empty transcription result');
            }
          } catch (transcribeError) {
            console.log('[SpeechService] ‚ö†Ô∏è Continuous transcription skipped:', transcribeError);
          }
        }
      } catch (error) {
        console.error('[SpeechService] ‚ùå Continuous transcription error:', error);
      }
    }, 5000); // Check every 5 seconds
  }

  /**
   * Stop continuous transcription
   */
  private stopContinuousTranscription(): void {
    if (this.transcriptionInterval) {
      clearInterval(this.transcriptionInterval);
      this.transcriptionInterval = null;
    }
  }

  /**
   * Start polling audio levels
   */
  private startAudioLevelPolling(): void {
    let sampleCount = 0;

    this.audioLevelInterval = setInterval(async () => {
      try {
        if (!NativeAudioModule || !this.isRecording) {
          return;
        }

        const levelResult = await NativeAudioModule.getAudioLevel();
        const level = levelResult.level || 0;

        // Log first 3 samples for debugging
        if (sampleCount < 3) {
          console.log(`[SpeechService] üéöÔ∏è Audio level sample ${sampleCount + 1}:`, level);
          sampleCount++;
        } else if (sampleCount === 3) {
          console.log('[SpeechService] üéöÔ∏è Audio level polling working (further logs suppressed)');
          sampleCount++;
        }

        if (this.audioLevelCallback) {
          this.audioLevelCallback(level);
        }
      } catch (error) {
        // Ignore polling errors
      }
    }, 100); // Poll every 100ms
  }

  /**
   * Stop polling audio levels
   */
  private stopAudioLevelPolling(): void {
    if (this.audioLevelInterval) {
      clearInterval(this.audioLevelInterval);
      this.audioLevelInterval = null;
    }
  }

  /**
   * Get recording status
   */
  getIsRecording(): boolean {
    return this.isRecording;
  }

  /**
   * Get recording duration in milliseconds
   */
  getRecordingDuration(): number {
    if (!this.isRecording) {
      return 0;
    }
    return Date.now() - this.recordingStartTime;
  }

  /**
   * Cleanup on service destruction
   */
  cleanup(): void {
    this.stopAudioLevelPolling();
    this.stopContinuousTranscription();
    if (this.isRecording) {
      this.cancelRecording();
    }
  }
}

// Export singleton instance
export const speechService = new SpeechService();
